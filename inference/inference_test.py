
import os
import json
from collections import Counter
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel
from chromadb import PersistentClient

# âœ… ê¸°ë³¸ ì„¤ì •
root_dir = "/home/mts/ssd_16tb/member/jks/tile_RAG_data/testphase2_dataset_v.0.1.1"
db_path = "/home/mts/ssd_16tb/member/jks/tile_RAG_data/vectorDB/tile_RAG_embedding_db_v0.1.0"
collection_name = "tile_embeddings"
top_k = 1
output_path = "predictions_v0.2.0.json"

# âœ… ëª¨ë¸, DB ì´ˆê¸°í™”
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch16").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch16")
client = PersistentClient(path=db_path)
collection = client.get_or_create_collection(name=collection_name)

# âœ… ì „ì²´ ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
results = []

# âœ… ìŠ¬ë¼ì´ë“œ ì „ì²´ ì²˜ë¦¬
slide_dirs = sorted([os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])

for slide_dir in slide_dirs:
    slide_id = os.path.basename(slide_dir)
    tile_paths = sorted([os.path.join(slide_dir, f) for f in os.listdir(slide_dir) if f.endswith(".jpg")])

    if not tile_paths:
        print(f"âš ï¸ íƒ€ì¼ ì—†ìŒ: {slide_id} â†’ ìŠ¤í‚µ")
        continue

    captions = []
    for path in tile_paths:
        try:
            image = Image.open(path).convert("RGB")
            inputs = processor(images=image, return_tensors="pt").to(device)
            with torch.no_grad():
                image_features = model.get_image_features(**inputs)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            embedding = image_features.squeeze().cpu().tolist()

            results_query = collection.query(
                query_embeddings=[embedding],
                n_results=top_k,
                include=["metadatas", "documents", "distances"]
            )

            metadata = results_query["metadatas"][0][0]
            caption = metadata.get("caption", "(ì—†ìŒ)")
            captions.append(caption)

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {path} â†’ {e}")

    if not captions:
        print(f"âš ï¸ ìº¡ì…˜ ì—†ìŒ: {slide_id}")
        continue

    # ğŸ” ìµœë¹ˆ ìº¡ì…˜ ì„ íƒ
    caption_counts = Counter(captions)
    most_common_caption, count = caption_counts.most_common(1)[0]

    print(f"\nâœ… ìµœì¢… ë³‘ë¦¬ ë¦¬í¬íŠ¸: {slide_id} (ë¹ˆë„ìˆ˜: {count}íšŒ)")
    print(f"ğŸ“„ {most_common_caption}")

    # ğŸ” ê²°ê³¼ ì €ì¥
    result_entry = {
        "id": f"{slide_id}.tiff",
        "report": most_common_caption
    }
    results.append(result_entry)

# âœ… JSON ì €ì¥
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print(f"\nğŸ“ ì „ì²´ ê²°ê³¼ JSON ì €ì¥ ì™„ë£Œ: {output_path}")

