import os
import json
import torch
import numpy as np
from PIL import Image, UnidentifiedImageError
from tqdm import tqdm
from chromadb import PersistentClient

import timm
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform

# =========================
# ğŸ”§ ê²½ë¡œ/ì„¤ì •
# =========================
root_dir = "/home/mts/ssd_16tb/member/jks/tile_RAG_data/train_set_v0.1.0"
groundtruth_json = "/home/mts/ssd_16tb/member/jks/reg2025_tile_RAG/embed/ground_truth_all.json"

db_path = "/home/mts/ssd_16tb/member/jks/tile_RAG_data/vectorDB/tile_RAG_embedding_db_v0.8.0"
collection_name = "tile_embeddings_virchow2"

model_dir = "/home/mts/ssd_16tb/member/jks/tile_RAG_data/virchow2"
safetensors_path = os.path.join(model_dir, "model.safetensors")
bin_path         = os.path.join(model_dir, "pytorch_model.bin")

BATCH_ADD = 4096
IMG_EXTS = (".jpg", ".jpeg", ".png")

# =========================
# âœ… ë””ë°”ì´ìŠ¤
# =========================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# =========================
# âœ… Virchow2(ë¹„ì „ ë°±ë³¸) ìƒì„± + ê°€ì¤‘ì¹˜ ë¡œë“œ
# - ViT-H/14 + SwiGLU + í™•ì¥ MLP(â‰ˆ6832)
# =========================
def build_virchow2_model() -> torch.nn.Module:
    """
    Virchow2 ì²´í¬í¬ì¸íŠ¸ì— ë§ì¶˜ ViT-H/14:
    - embed_dim=1280, depth=32, heads=16
    - SwiGLU + hidden_dim=6832 (â†’ mlp_ratio ì •í™•íˆ 5.3375)
    - SiLU, LayerScale init=1e-5
    """
    embed_dim = 1280
    hidden_dim = 6832                        # â˜… ì²´í¬í¬ì¸íŠ¸ ê¸°ì¤€
    mlp_ratio_exact = hidden_dim / embed_dim # = 5.3375

    model = timm.create_model(
        "vit_huge_patch14_224",
        pretrained=False,
        num_classes=0,
        mlp_layer=timm.layers.SwiGLUPacked,
        act_layer=torch.nn.SiLU,
        mlp_ratio=mlp_ratio_exact,   # â˜… ë°˜ì˜¬ë¦¼ ì—†ì´ ì •í™•íˆ 5.3375
        init_values=1e-5,
        no_embed_class=True,
    )
    model.reset_classifier(0)
    model.eval().to(device)

    # === ê°€ì¤‘ì¹˜ ë¡œë“œ ===
    state = None
    if os.path.exists(safetensors_path):
        try:
            from safetensors.torch import load_file
            state = load_file(safetensors_path)
            print(f"â–¶ Loaded safetensors: {safetensors_path}")
        except Exception as e:
            print(f"âš ï¸ safetensors ë¡œë“œ ì‹¤íŒ¨ â†’ {e}")

    if state is None and os.path.exists(bin_path):
        try:
            state = torch.load(bin_path, map_location="cpu")
            print(f"â–¶ Loaded bin: {bin_path}")
        except Exception as e:
            raise RuntimeError(f"pytorch_model.bin ë¡œë“œ ì‹¤íŒ¨: {e}")

    if state is None:
        raise FileNotFoundError("Virchow2 ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    clean = {k.replace("module.", ""): v for k, v in state.items()}

    # pos_embed shape ë¶ˆì¼ì¹˜ ì‹œ ì œê±°
    if "pos_embed" in clean:
        try:
            if hasattr(model, "pos_embed") and clean["pos_embed"].shape != model.pos_embed.shape:
                print("âš ï¸ pos_embed shape ë¶ˆì¼ì¹˜ â†’ í‚¤ ì œê±°")
                del clean["pos_embed"]
        except Exception:
            pass

    missing, unexpected = model.load_state_dict(clean, strict=False)
    print(f"â†’ load_state_dict: missing={len(missing)}, unexpected={len(unexpected)}")
    if missing:   print("  (info) missing ì˜ˆ:", missing[:6])
    if unexpected: print("  (info) unexpected ì˜ˆ:", unexpected[:6])

    # (ì„ íƒ) ì²« ë¸”ë¡ MLP í¬ê¸° í™•ì¸ ë¡œê·¸
    try:
        fc1_w = model.blocks[0].mlp.fc1.weight
        fc2_w = model.blocks[0].mlp.fc2.weight
        print(f"[CHECK] fc1: {tuple(fc1_w.shape)}, fc2: {tuple(fc2_w.shape)} "
              f"(ê¸°ëŒ€: fc1={(hidden_dim*1, embed_dim)}, fc2={(embed_dim, hidden_dim//2)})")
    except Exception:
        pass

    return model


# =========================
# âœ… ì „ì²˜ë¦¬(transform)
# =========================
def build_transform(model: torch.nn.Module):
    config = resolve_data_config({}, model=model)
    transform = create_transform(**config)
    return transform


# =========================
# âœ… ì„ë² ë”© í•¨ìˆ˜
# =========================
@torch.no_grad()
def get_embedding(img_path: str, transform, model) -> np.ndarray:
    try:
        with Image.open(img_path) as img:
            img = img.convert("RGB")
    except (UnidentifiedImageError, OSError) as e:
        raise RuntimeError(f"ì´ë¯¸ì§€ ì—´ê¸° ì‹¤íŒ¨: {img_path} â†’ {e}")

    x = transform(img).unsqueeze(0).to(device)       # [1, 3, 224, 224]
    feat = model.forward_features(x)                  # [1, N, D] ë˜ëŠ” [1, D]
    if isinstance(feat, (list, tuple)):
        feat = feat[0]
    if feat.ndim == 3:                                # [B, N, D] â†’ íŒ¨ì¹˜ í† í° í‰ê· í’€ë§
        feat = feat.mean(dim=1)
    emb = torch.nn.functional.normalize(feat, dim=-1)  # L2 normalize
    return emb.squeeze(0).cpu().numpy()              # [D]


# =========================
# âœ… ë°°ì¹˜ add ìœ í‹¸
# =========================
def flush_batch(collection, ids, embs, metas):
    if not ids:
        return
    collection.add(ids=ids, embeddings=embs, metadatas=metas)
    ids.clear(); embs.clear(); metas.clear()


# =========================
# âœ… ë©”ì¸
# =========================
def main():
    # ëª¨ë¸/ì „ì²˜ë¦¬
    model = build_virchow2_model()
    transform = build_transform(model)

    # Groundtruth ë¡œë“œ
    with open(groundtruth_json, "r") as f:
        groundtruth = json.load(f)
    slide_to_caption = {item["id"].replace(".tiff", ""): item["report"] for item in groundtruth}

    gt_slide_ids = set(slide_to_caption.keys())
    tile_slide_ids = set([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])
    matched_ids = sorted(list(gt_slide_ids & tile_slide_ids))

    # ChromaDB
    client = PersistentClient(path=db_path)
    collection = client.get_or_create_collection(name=collection_name)

    for slide_id in matched_ids:
        slide_dir = os.path.join(root_dir, slide_id)
        files = sorted([f for f in os.listdir(slide_dir) if f.lower().endswith(IMG_EXTS)])
        if not files:
            print(f"âš ï¸ íƒ€ì¼ ì—†ìŒ: {slide_dir}")
            continue

        # ê¸°ì¡´ ë™ì¼ slide_id ë©”íƒ€ ì‚­ì œ(ë¦¬ë¹Œë“œ)
        try:
            collection.delete(where={"slide_id": slide_id})
        except Exception as e:
            print(f"âš ï¸ delete ì‹¤íŒ¨(ë¬´ì‹œ ê°€ëŠ¥): {slide_id} â†’ {e}")

        caption = slide_to_caption[slide_id]
        ids, embeddings, metadatas = [], [], []

        pbar = tqdm(files, desc=f"[Virchow2] {slide_id}", ncols=100)
        for fname in pbar:
            img_path = os.path.join(slide_dir, fname)
            try:
                emb = get_embedding(img_path, transform, model)
            except Exception as e:
                print(f"âŒ ì„ë² ë”© ì‹¤íŒ¨: {img_path} â†’ {e}")
                continue

            tile_id = f"{slide_id}_{fname}"
            ids.append(tile_id)
            embeddings.append(emb.tolist())
            metadatas.append({
                "slide_id": slide_id,
                "tile_name": fname,
                "tile_path": img_path,
                "caption": caption
            })

            if len(ids) >= BATCH_ADD:
                flush_batch(collection, ids, embeddings, metadatas)

        # ë‚¨ì€ ë°°ì¹˜ ë§ˆë¬´ë¦¬
        flush_batch(collection, ids, embeddings, metadatas)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {slide_id}")

    print("ğŸ‰ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ")


if __name__ == "__main__":
    main()
