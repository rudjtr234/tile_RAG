import os
import json
from PIL import Image
import torch
import numpy as np
from tqdm import tqdm
from chromadb import PersistentClient
import timm
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform
import torch.multiprocessing as mp
from math import ceil

# =========================
# í™˜ê²½ ê¸°ë³¸
# =========================
DB_PATH = "/home/mts/ssd_16tb/member/jks/tile_RAG_data/vectorDB/tile_RAG_embedding_db_v0.4.2"
COLL_NAME = "tile_embeddings_UNI2_AUG"  # ê¸°ì¡´ê³¼ ë™ì¼ ì´ë¦„ ì‚¬ìš©
ROOT_DIR  = "/home/mts/ssd_16tb/member/jks/tile_RAG_data/train_set_v0.1.0"
GT_PATH   = "/home/mts/ssd_16tb/member/jks/reg2025_tile_RAG/embed/ground_truth_all.json"

# =========================
# ëª¨ë¸/ì „ì²˜ë¦¬ ì‘ì„±ì í•¨ìˆ˜(í”„ë¡œì„¸ìŠ¤ë³„ ì´ˆê¸°í™”)
# =========================
def build_model_and_transform(device):
    timm_kwargs = {
        "img_size": 224, "patch_size": 14, "depth": 24, "num_heads": 24,
        "init_values": 1e-5, "embed_dim": 1536, "mlp_ratio": 2.66667*2,
        "num_classes": 0, "no_embed_class": True,
        "mlp_layer": timm.layers.SwiGLUPacked, "act_layer": torch.nn.SiLU,
        "reg_tokens": 8, "dynamic_img_size": True,
    }
    model = timm.create_model("hf-hub:MahmoodLab/UNI2-h", pretrained=True, **timm_kwargs).to(device)
    model.eval()
    data_cfg = resolve_data_config({}, model=model)
    transform = create_transform(**data_cfg)
    return model, transform

# ======================
# ê³µìš© ìœ í‹¸
# ======================
def l2_normalize(x: torch.Tensor, dim: int = -1, eps: float = 1e-12) -> torch.Tensor:
    return x / (x.norm(dim=dim, keepdim=True) + eps)

@torch.inference_mode()
def _forward_batch(model, tensor: torch.Tensor, device, use_fp16: bool) -> torch.Tensor:
    ctx = (torch.autocast(device_type="cuda", dtype=torch.float16) if (use_fp16 and device.type=="cuda") else torch.no_grad())
    with ctx:
        feats = model(tensor)           # [N, D]
    return l2_normalize(feats, dim=-1)  # [N, D]

# ======================
# 2x2 ë©€í‹°í¬ë¡­(ê° í¬ë¡­ ì„ë² ë”© ë°˜í™˜)
# ======================
@torch.inference_mode()
def embed_2x2(img_path: str, model, transform, device, use_fp16: bool):
    image = Image.open(img_path).convert("RGB")
    W, H = image.size
    gx = gy = 2

    # ê· ë“± 2x2 ë¶„í• 
    cw, ch = W // gx, H // gy
    crops = []
    for ry in range(gy):
        for rx in range(gx):
            x0, y0 = rx * cw, ry * ch
            x1, y1 = min(x0 + cw, W), min(y0 + ch, H)
            crops.append(image.crop((x0, y0, x1, y1)))

    batch = torch.stack([transform(c) for c in crops], dim=0).to(device)  # [4,3,224,224]
    feats = _forward_batch(model, batch, device, use_fp16)                # [4, D]
    return [feats[i].detach().cpu().numpy() for i in range(feats.shape[0])]

from itertools import count

# ======================
# íƒ€ì¼ ë””ë ‰í† ë¦¬ ì €ì¥ (ids í¬í•¨, ë©”íƒ€ë°ì´í„° ìµœì†Œ)
# ======================
def embed_and_store(img_dir: str, slide_id: str, collection, slide_to_caption, model, transform, device, use_fp16: bool):
    files = [f for f in sorted(os.listdir(img_dir))
             if f.lower().endswith((".jpg", ".jpeg")) and os.path.isfile(os.path.join(img_dir, f))]
    if not files:
        print(f"âš ï¸ íƒ€ì¼ ì—†ìŒ: {img_dir}")
        return

    BATCH_LIMIT = 4096
    embeddings, metadatas, ids = [], [], []
    local_counter = count(1)  # ìŠ¬ë¼ì´ë“œ ë‚´ë¶€ ì¼ë ¨ë²ˆí˜¸: 1,2,3,...

    tiff_id = f"{slide_id}.tiff"
    caption = slide_to_caption.get(slide_id, "")

    for fname in tqdm(files, desc=f"[{slide_id}] Embedding", leave=False):
        img_path = os.path.join(img_dir, fname)
        base = os.path.splitext(fname)[0]  # ë¶€ëª¨ 896 íƒ€ì¼ ì´ë¦„
        try:
            emb_list = embed_2x2(img_path, model, transform, device, use_fp16)  # 4ê°œ ì„ë² ë”©
            for emb in emb_list:
                embeddings.append(emb.tolist())
                metadatas.append({
                    "slide_id": slide_id,
                    "tiff_id": tiff_id,
                    "tile_name": base,
                    "caption": caption
                })
                # âœ… ê³ ìœ  ID: ìŠ¬ë¼ì´ë“œ ì´ë¦„ + ì¼ë ¨ë²ˆí˜¸ (ì˜ˆ: PIT_01_00001_01_00000001)
                n = next(local_counter)
                ids.append(f"{slide_id}_{n:08d}")

        except Exception as e:
            print(f"  â†³ ê±´ë„ˆëœ€(ì—ëŸ¬): {fname} â†’ {e}")
            continue

        # ì¤‘ê°„ flush
        if len(embeddings) >= BATCH_LIMIT:
            collection.add(ids=ids, embeddings=embeddings, metadatas=metadatas)
            embeddings, metadatas, ids = [], [], []

    # ì”ì—¬ flush
    if embeddings:
        collection.add(ids=ids, embeddings=embeddings, metadatas=metadatas)

# ======================
# ì›Œì»¤(í”„ë¡œì„¸ìŠ¤) í•¨ìˆ˜: GPU í•˜ë‚˜ ë‹´ë‹¹
# ======================
def worker(rank: int, slide_ids_chunk, world_size: int):
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if torch.cuda.is_available():
        torch.cuda.set_device(rank)
        device = torch.device(f"cuda:{rank}")
        use_fp16 = True
    else:
        device = torch.device("cpu")
        use_fp16 = False

    # í”„ë¡œì„¸ìŠ¤ë³„ ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
    model, transform = build_model_and_transform(device)
    chroma_client = PersistentClient(path=DB_PATH)
    collection = chroma_client.get_or_create_collection(name=COLL_NAME)

    # GT ë¡œë“œ (í”„ë¡œì„¸ìŠ¤ë³„ ë…ë¦½ ë¡œë“œ)
    with open(GT_PATH, "r") as f:
        groundtruth = json.load(f)
    slide_to_caption = {item["id"].replace(".tiff", ""): item["report"] for item in groundtruth}

    # ì²˜ë¦¬ ë£¨í”„
    for slide_id in slide_ids_chunk:
        slide_dir = os.path.join(ROOT_DIR, slide_id)
        try:
            # ë™ì¼ slide_id ê¸°ì¡´ ë°ì´í„° ì œê±°(ë™ì‹œì— ì—¬ëŸ¬ ì›Œì»¤ê°€ ê°™ì€ slideë¥¼ ë§Œì§€ì§€ ì•Šìœ¼ë¯€ë¡œ ì•ˆì „)
            collection.delete(where={"slide_id": slide_id})
            embed_and_store(slide_dir, slide_id, collection, slide_to_caption, model, transform, device, use_fp16)
            print(f"[GPU{rank}] âœ… ì €ì¥ ì™„ë£Œ: {slide_id}")
        except Exception as e:
            print(f"[GPU{rank}] âŒ ì˜¤ë¥˜ ë°œìƒ: {slide_id} â†’ {e}")

# ======================
# ë©”ì¸: ìŠ¬ë¼ì´ë“œ ë¶„í•  í›„ ë©€í‹° GPU ì‹¤í–‰
# ======================
def split_evenly(items, n_parts):
    n = len(items)
    if n_parts <= 1:
        return [items]
    step = ceil(n / n_parts)
    return [items[i:i+step] for i in range(0, n, step)]

def main():
    # GTì™€ ìŠ¬ë¼ì´ë“œ ëª©ë¡ ì¤€ë¹„(ë©”ì¸ í”„ë¡œì„¸ìŠ¤)
    with open(GT_PATH, "r") as f:
        groundtruth = json.load(f)
    slide_to_caption = {item["id"].replace(".tiff", ""): item["report"] for item in groundtruth}

    gt_slide_ids = set(slide_to_caption.keys())
    tile_slide_ids = set([d for d in os.listdir(ROOT_DIR) if os.path.isdir(os.path.join(ROOT_DIR, d))])
    matched_ids = sorted(list(gt_slide_ids & tile_slide_ids))

    if not matched_ids:
        print("âš ï¸ ì²˜ë¦¬í•  ìŠ¬ë¼ì´ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # GPU ìˆ˜
    num_gpus = torch.cuda.device_count()
    if num_gpus <= 1:
        # ì‹±ê¸€ í”„ë¡œì„¸ìŠ¤ ê²½ë¡œ
        print("ğŸ”§ Single GPU/CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        worker(rank=0, slide_ids_chunk=matched_ids, world_size=1)
        return

    print(f"ğŸš€ Multi-GPU ì‹¤í–‰: GPU {num_gpus}ê°œ")
    chunks = split_evenly(matched_ids, num_gpus)

    # spawn
    mp.set_start_method("spawn", force=True)
    procs = []
    for rank in range(num_gpus):
        chunk = chunks[rank] if rank < len(chunks) else []
        p = mp.Process(target=worker, args=(rank, chunk, num_gpus), daemon=False)
        p.start()
        procs.append(p)
    for p in procs:
        p.join()
    print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ")

# ======================
# ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# ======================
if __name__ == "__main__":
    main()
