
import os
import json
from PIL import Image
import torch
import numpy as np
from tqdm import tqdm
from chromadb import PersistentClient
import os, timm, torch
from timm.data import resolve_data_config
from timm.data.transforms_factory import create_transform


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

timm_kwargs = {
    "img_size": 224,
    "patch_size": 14,
    "depth": 24,
    "num_heads": 24,
    "init_values": 1e-5,
    "embed_dim": 1536,
    "mlp_ratio": 2.66667*2,
    "num_classes": 0,
    "no_embed_class": True,
    "mlp_layer": timm.layers.SwiGLUPacked,
    "act_layer": torch.nn.SiLU,
    "reg_tokens": 8,
    "dynamic_img_size": True,
}

model = timm.create_model(
    "hf-hub:MahmoodLab/UNI2-h",
    pretrained=True,
    **timm_kwargs,
).to(device)
model.eval()

# ê¶Œì¥ ì „ì²˜ë¦¬ (ë°˜ë“œì‹œ ì´ê±¸ë¡œ ì…ë ¥ ë§Œë“¤ê¸°)
transform = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))

# ======================
# âœ… ChromaDB ì„¤ì •
# ======================
chroma_client = PersistentClient(
    path="/home/mts/ssd_16tb/member/jks/tile_RAG_data/vectorDB/tile_RAG_embedding_db_v0.4.1"
)
collection = chroma_client.get_or_create_collection(name="tile_embeddings_final_UNI2")

# ======================
# âœ… Groundtruth JSON ë¶ˆëŸ¬ì˜¤ê¸°
# ======================
with open("/home/mts/ssd_16tb/member/jks/reg2025_tile_RAG/embed/ground_truth_all.json", "r") as f:
    groundtruth = json.load(f)

# âœ… ìŠ¬ë¼ì´ë“œ ID â†’ ìº¡ì…˜ ë§µ
slide_to_caption = {
    item["id"].replace(".tiff", ""): item["report"] for item in groundtruth
}

# âœ… ê³µí†µëœ ìŠ¬ë¼ì´ë“œ IDë§Œ ì¶”ì¶œ
root_dir = "/home/mts/ssd_16tb/member/jks/tile_RAG_data/reg_2025_final_v.0.2.0"
gt_slide_ids = set(slide_to_caption.keys())
tile_slide_ids = set([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])
matched_ids = sorted(list(gt_slide_ids & tile_slide_ids))

# ======================
# âœ… ì„ë² ë”© í•¨ìˆ˜ (UNI2-h)
#   - ì¶œë ¥: (1536,) numpy
#   - L2 ì •ê·œí™”
# ======================
@torch.inference_mode()
def get_embedding(img_path: str) -> np.ndarray:
    image = Image.open(img_path).convert("RGB")
    tensor = transform(image).unsqueeze(0).to(device)  # [1, 3, H, W] (ê¸°ë³¸ 224)
    # ì†ë„ í–¥ìƒ: FP16 ìë™ ìºìŠ¤íŠ¸ (CUDAì¼ ë•Œë§Œ)
    use_amp = (device.type == "cuda")
    ctx = torch.autocast("cuda", dtype=torch.float16) if use_amp else torch.no_grad()
    with ctx:
        feat = model(tensor)  # [1, 1536]
    feat = torch.nn.functional.normalize(feat, dim=-1)  # L2 ì •ê·œí™”
    return feat.squeeze(0).detach().cpu().numpy()

# ======================
# âœ… íƒ€ì¼ ë””ë ‰í† ë¦¬ ë‹¨ìœ„ ì €ì¥ í•¨ìˆ˜
# ======================
def embed_and_store(img_dir: str, slide_id: str, caption: str):
    files = sorted([f for f in os.listdir(img_dir) if f.lower().endswith((".jpg", ".jpeg"))])
    if not files:
        print(f"âš ï¸ íƒ€ì¼ ì—†ìŒ: {img_dir}")
        return

    embeddings, ids, metadatas = [], [], []

    for fname in tqdm(files, desc=f"Embedding {slide_id}"):
        img_path = os.path.join(img_dir, fname)
        emb = get_embedding(img_path)  # (1536,)
        embeddings.append(emb.tolist())
        tile_id = f"{slide_id}_{fname}"
        ids.append(tile_id)
        metadatas.append({
            "slide_id": slide_id,
            "tile_name": fname,
            "tile_path": img_path,
            "caption": caption
        })

    collection.add(embeddings=embeddings, ids=ids, metadatas=metadatas)

# ======================
# âœ… ë©”ì¸ ë£¨í”„
# ======================
if __name__ == "__main__":
    for slide_id in matched_ids:
        slide_dir = os.path.join(root_dir, slide_id)

        try:
            # ğŸ”¥ ê¸°ì¡´ slide_id ë°ì´í„° ì‚­ì œ í›„ ì¬ì‚½ì…
            collection.delete(where={"slide_id": slide_id})
            embed_and_store(slide_dir, slide_id, slide_to_caption[slide_id])
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {slide_id}")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {slide_id} â†’ {e}")






